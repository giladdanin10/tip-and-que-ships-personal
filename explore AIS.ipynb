{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "from display_aux import *\n",
    "# from vessels import VESSELES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {}\n",
    "# params['input_csv_file_name_full'] = 'C:\\\\gilad\\\\work\\\\tip_and_que\\\\data\\\\AIS\\\\TipandCue_DataSample_CSV\\\\exactEarth_historical_data_02_2023.csv'\n",
    "params['input_csv_file_name_full'] = 'debug_data_base.csv'\n",
    "\n",
    "params['min_date'] = None\n",
    "params['max_date'] = None\n",
    "params['columns_list_keep'] = ['Time','MMSI','IMO','Vessel_Name','Ship_Type','Longitude','Latitude','Message_ID','Accuracy','Heading','COG','Fixing_device','Destination_ID','offset1','Offset_2','Offset_3','Offset_4','ATON_type','ATON_name','GNSS_status']\n",
    "params['filter_vessels_df_dic'] = {\n",
    "          'max_time_diff[mins]':['<=',30]\n",
    "          }\n",
    "params['reload'] = False\n",
    "params['export_to_excel'] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, filter_dic):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame based on a dictionary of column filters.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be filtered.\n",
    "    filter_dic (dict): A dictionary where keys are column names and values are tuples.\n",
    "                       Each tuple contains an operator as the first element and the filter value(s) as the second element.\n",
    "                       Supported operators: '==', '!=', '<', '<=', '>', '>=', 'between'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The filtered DataFrame or an empty DataFrame if any column does not exist.\n",
    "\n",
    "    Example Usage:\n",
    "    inf_df = pd.DataFrame({\n",
    "        'MMSI': [123456789, 987654321, 192837465],\n",
    "        'Vessel_Name': ['Vessel A', 'Vessel B', 'Vessel C'],\n",
    "        'Latitude': [34.5, 45.6, 56.7],\n",
    "        'Longitude': [-123.4, -134.5, -145.6]\n",
    "    })\n",
    "\n",
    "    filter_dic = {\n",
    "        'Latitude': ('between', (40.0, 50.0)),  # Applying a 'between' filter for Latitude\n",
    "        'Longitude': ('<=', -134.5),  # Applying a '<=' filter for Longitude\n",
    "        'Vessel_Name': ('==', ['Vessel A', 'Vessel C']),  # Applying an '==' filter for Vessel_Name\n",
    "        'Nonexistent_Column': ('==', 'SomeValue')  # Nonexistent column\n",
    "    }\n",
    "\n",
    "    filtered_df = filter_df(inf_df, filter_dic)\n",
    "    print(filtered_df)\n",
    "    \"\"\"\n",
    "    for column, (operator, value) in filter_dic.items():\n",
    "        if column not in df.columns:\n",
    "            print(f\"Error: Column '{column}' does not exist in the DataFrame. Existing columns: {list(df.columns)}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame\n",
    "        \n",
    "        if operator == '==':\n",
    "            if isinstance(value, list):\n",
    "                df = df[df[column].isin(value)]\n",
    "            else:\n",
    "                df = df[df[column] == value]\n",
    "        elif operator == '!=':\n",
    "            df = df[df[column] != value]\n",
    "        elif operator == '<':\n",
    "            df = df[df[column] < value]\n",
    "        elif operator == '<=':\n",
    "            df = df[df[column] <= value]\n",
    "        elif operator == '>':\n",
    "            df = df[df[column] > value]\n",
    "        elif operator == '>=':\n",
    "            df = df[df[column] >= value]\n",
    "        elif operator == 'between':\n",
    "            if isinstance(value, tuple) and len(value) == 2:\n",
    "                lower_bound, upper_bound = value\n",
    "                df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "            else:\n",
    "                raise ValueError(f\"Value for 'between' must be a tuple of two elements: {value}\")\n",
    "            \n",
    "            if (lower_bound>upper_bound):\n",
    "                print(f'lower bound ({lower_bound}) is higher than upper bound ({upper_bound})')\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported operator: {operator}\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def repeat_single_value_in_column (df,value,column_name,to_print=False):\n",
    "    if not isinstance(value, list) and not isinstance(value, np.ndarray):\n",
    "        value = [value]\n",
    "\n",
    "    # print(value)        \n",
    "    if (len(value) != 1):\n",
    "        if (to_print):\n",
    "            print(f'value is not unique:{value}')\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df[column_name] = np.repeat(value,df.shape[0])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manage data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_vessel_data(vessel_data_dic,vessel_MMSI,to_print=False):\n",
    "    if (1):\n",
    "    # try:\n",
    "        vessel_data = vessel_data_dic[vessel_MMSI]\n",
    "        \n",
    "        # repeat missing ID values\n",
    "        ID_columns = ['IMO','Vessel_Name','Ship_Type']\n",
    "        for ID_column in ID_columns:\n",
    "            data = vessel_data[ID_column].loc[vessel_data[ID_column].notna()]\n",
    "            data = data.unique()\n",
    "            if (isinstance(data,str)):\n",
    "                data = data.strip()\n",
    "            # print(vessel_data.shape)\n",
    "            vessel_data = repeat_single_value_in_column(vessel_data,data,ID_column)\n",
    "            # print(vessel_data.shape)\n",
    "            \n",
    "            if (vessel_data.empty):\n",
    "                if (to_print is True):\n",
    "                    print(f'failed to create data base for vessel_MMSI={vessel_MMSI}')\n",
    "\n",
    "                return vessel_data\n",
    "\n",
    "\n",
    "        # take only the lines where there is a Longitude\n",
    "        # print(vessel_data.shape)\n",
    "\n",
    "        vessel_data = vessel_data[vessel_data['Longitude'].notna()]\n",
    "\n",
    "# handle exponent represntations \n",
    "        vessel_data.loc[:, 'Latitude'] = vessel_data['Latitude'].apply(convert_to_float)\n",
    "        vessel_data.loc[:, 'Longitude'] = vessel_data['Longitude'].apply(convert_to_float)\n",
    "        \n",
    "        # print(vessel_data.shape)\n",
    " \n",
    "        # sort data by time\n",
    "        vessel_data = vessel_data.sort_values(by='Time')\n",
    "        # print('sucess')\n",
    "    # except:\n",
    "    #     vessel_data = pd.DataFrame()\n",
    "    #     # sys.exit(1)   \n",
    "    return vessel_data\n",
    "\n",
    "\n",
    "\n",
    "# vessel_MMSI = vessel_data_info['single'][100]\n",
    "# vesel_data = get_vessel_data(vessel_data_dic,vessel_MMSI)\n",
    "\n",
    "# vessel_data.head()\n",
    "\n",
    "\n",
    "def get_vessel_data_stats(vessel_data):\n",
    "    stats_dic = {\n",
    "        'len': [vessel_data.shape[0]],  # Scalar value wrapped in a list\n",
    "        'min_time':get_min_max_dates(vessel_data)[0],\n",
    "        'max_time':get_min_max_dates(vessel_data)[1],\n",
    "        'total_time':max(vessel_data['Time'])- min(vessel_data['Time']),\n",
    "        'min_time_diff[mins]': round(np.min(time_diff_convert(vessel_data['Time'].diff()))),\n",
    "        'max_time_diff[mins]': round(np.max(time_diff_convert(vessel_data['Time'].diff()))),\n",
    "        'mean_time_diff[mins]': round(np.mean(time_diff_convert(vessel_data['Time'].diff()))),\n",
    "        'min_Longitude':(min(vessel_data['Longitude'])),\n",
    "        'max_Longitude':(max(vessel_data['Longitude'])),\n",
    "        'min_Latitude':(min(vessel_data['Latitude'])),\n",
    "        'max_Latitude':(max(vessel_data['Latitude'])),\n",
    "    }\n",
    "    stats_dic['span_Longitude']  = stats_dic['max_Longitude']-stats_dic['min_Longitude']\n",
    "    stats_dic['span_Latitude']  = stats_dic['max_Latitude']-stats_dic['min_Latitude']\n",
    "\n",
    "        # 'diff_Latitude':max(vessel_data['Latitude'])-min(vessel_data['Latitude'])\n",
    "\n",
    "\n",
    "    return stats_dic\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_vessels_df(vessel_data_dic,MMSI_list,min_data_len_thresh = 2):\n",
    "#     inf_df = pd.DataFrame()\n",
    "#     vessel_MMSI_prob = []\n",
    "\n",
    "#     for i,vessel_MMSI in enumerate(MMSI_list):\n",
    "#         if (i%1000==0):\n",
    "#             print(f'proccessing MMSI {i} out of {len(vessel_data_info_list)}')\n",
    "#         vessel_data = get_vessel_data(vessel_data_dic,vessel_MMSI)\n",
    "#         if (vessel_data.shape[0]<min_data_len_thresh):\n",
    "#             vessel_MMSI_prob.append(vessel_MMSI)\n",
    "#         else:\n",
    "#             vessels_df_line = pd.DataFrame(get_vessel_data_stats(vessel_data), index=[vessel_MMSI])  # Providing index explicitly\n",
    "#             inf_df = pd.concat([inf_df,vessels_df_line])\n",
    "\n",
    "#     inf_df = inf_df.sort_values(by='len',ascending=False)\n",
    "#     print(inf_df)\n",
    "#     return vessles_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File handling functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_df(csv_file_path, save_path,reload = False):\n",
    "    if os.path.exists(save_path) and reload==False:\n",
    "        print(f\"Loading DataFrame from {save_path}\")\n",
    "        df = pd.read_pickle(save_path)\n",
    "    else:\n",
    "        print(f\"Reading CSV file from {csv_file_path}\")\n",
    "        df = pd.read_csv(csv_file_path, low_memory=False)\n",
    "        print(f\"Saving DataFrame to {save_path}\")\n",
    "        df.to_pickle(save_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def save_vessel_data_to_geojson(vessel_data, file_path = './data', file_name=None):\n",
    "    \"\"\"\n",
    "    Save latitude and longitude data from a DataFrame to a GeoJSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    - vessel_data: pandas DataFrame containing 'Latitude' and 'Longitude' columns.\n",
    "    - file_path: Directory path where the GeoJSON file will be saved.\n",
    "    - file_name: Name of the GeoJSON file (without the .geojson extension).\n",
    "    \"\"\"\n",
    "\n",
    "    if (file_name is None):\n",
    "        file_name = vessel_data['Vessel_Name'].iloc[0]\n",
    "\n",
    "    file_name = file_name.rstrip()\n",
    "\n",
    "    # Create a geometry column with Point objects\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(vessel_data['Longitude'], vessel_data['Latitude'])]\n",
    "\n",
    "    # Create a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(vessel_data, geometry=geometry, crs='EPSG:4326')  # Assuming WGS84 projection\n",
    "\n",
    "    # Save to GeoJSON file\n",
    "    file_name_geojson = f\"{file_path}/{file_name}.geojson\"\n",
    "    gdf.to_file(file_name_geojson, driver='GeoJSON')\n",
    "\n",
    "\n",
    "def save_vessels_data_to_geojson(vessels_df_info,vessel_data_dic,file_path):\n",
    "    for i in range(inf_df.shape[0]):\n",
    "        try:\n",
    "            if (i % 10==0):\n",
    "                print(f'saving {i} files out of {inf_df.shape[0]}')\n",
    "            vessel_data = get_vessel_data(vessel_data_dic,inf_df.index[i])\n",
    "            save_vessel_data_to_geojson(vessel_data,file_path)\n",
    "        except:\n",
    "            print(f'could not export MMSI={inf_df.index[i]} to jason')\n",
    "    print(f'saved {i} files in {file_path}')\n",
    "    return   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_df_by_date(df, min_date, max_date, time_column='Time', date_format='%Y-%m-%d %H:%M:%S'):\n",
    "    \"\"\"\n",
    "    Function to filter a DataFrame based on a time column and specified date range.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    \n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - min_date (str): The minimum date as a string.\n",
    "    - max_date (str): The maximum date as a string.\n",
    "    - time_column (str): The name of the column containing time data in the specified format.\n",
    "    - date_format (str): The format of the date strings in the time column and min_date, max_date.\n",
    "    \n",
    "    Returns:\n",
    "    - filtered_df (pd.DataFrame): The DataFrame filtered by the specified date range.\n",
    "    \"\"\"\n",
    "    # Convert the Time column to datetime\n",
    "    df[time_column] = pd.to_datetime(df[time_column], format=date_format)\n",
    "\n",
    "    if (min_date is None):\n",
    "        min_date = min(df[time_column])\n",
    "\n",
    "    if (max_date is None):\n",
    "        max_date = max(df[time_column])\n",
    "\n",
    "\n",
    "    \n",
    "    # Convert min_date and max_date to datetime\n",
    "    min_date = pd.to_datetime(min_date, format=date_format)\n",
    "    max_date = pd.to_datetime(max_date, format=date_format)\n",
    "    \n",
    "    # Filter the DataFrame based on the date range\n",
    "    filtered_df = df[(df[time_column] >= min_date) & (df[time_column] <= max_date)]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Define the minimum and maximum dates\n",
    "# min_date = '2023-02-01 00:00:01'\n",
    "# max_date = '2023-02-02 00:00:01'\n",
    "\n",
    "# # Filter the DataFrame based on the date range\n",
    "# df = filter_df_by_date(df, min_date, max_date)\n",
    "\n",
    "# get_min_max_dates(df)\n",
    "\n",
    "\n",
    "\n",
    "def time_diff_convert(time_diff,units='mins',to_round=True):\n",
    "    if (not isinstance(time_diff,pd.core.series.Series)):\n",
    "        is_series = False\n",
    "        time_diff = pd.Series(time_diff)\n",
    "    else:\n",
    "        is_series = True\n",
    "\n",
    "    if (units == 'secs'):        \n",
    "        time_diff_mod = time_diff.apply(lambda x: x.total_seconds()) \n",
    "    \n",
    "    if (units == 'mins'):        \n",
    "        time_diff_mod = time_diff.apply(lambda x: x.total_seconds() / 60) \n",
    "\n",
    "    if (units == 'hours'):        \n",
    "        time_diff_mod = time_diff.apply(lambda x: x.total_seconds() / 3600) \n",
    "\n",
    "    if (to_round):\n",
    "        time_diff_mod = round(time_diff_mod)\n",
    "\n",
    "    if (not is_series):\n",
    "        time_diff_mod = time_diff_mod.values[0]        \n",
    "    return time_diff_mod\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def convert_time_format(df, time_column, current_format, output_format):\n",
    "    \"\"\"\n",
    "    Function to convert the time format of a specified column in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - time_column (str): The name of the column containing time data.\n",
    "    - current_format (str): The current format of the time data in the column.\n",
    "    - output_format (str): The desired output format for the tim data.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pd.DataFrame): The DataFrame with the time column converted to the desired format.\n",
    "    \"\"\"\n",
    "    # Convert the Time column to datetime using the current format\n",
    "    df[time_column] = pd.to_datetime(df[time_column], format=current_format)\n",
    "    \n",
    "    # Convert the datetime to the desired output format\n",
    "    df[time_column] = df[time_column].dt.strftime(output_format)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_min_max_dates(df, time_column='Time',input_format = '%Y-%m-%d %H:%M:%S',output_format='%Y-%m-%d %H:%M:%S'):\n",
    "    \"\"\"\n",
    "    Function to get the minimum and maximum dates from a DataFrame's time column.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - time_column (str): The name of the column containing time data in '%Y-%m-%d %H:%M:%S' format.\n",
    "    - output_format (str): The desired output datetime format.\n",
    "    \n",
    "    Returns:\n",
    "    - min_date (str): The minimum date in the desired format.\n",
    "    - max_date (str): The maximum date in the desired format.\n",
    "    \"\"\"\n",
    "    # # Convert the Time column to datetime\n",
    "    # df[time_column] = pd.to_datetime(df[time_column], format=input_format)\n",
    "    \n",
    "    # Get the minimum and maximum dates\n",
    "    min_date = df[time_column].min().strftime(output_format)\n",
    "    max_date = df[time_column].max().strftime(output_format)\n",
    "    \n",
    "    return min_date, max_date\n",
    "\n",
    "\n",
    "\n",
    "# # Get the minimum and maximum dates in the desired format\n",
    "# min_date, max_date = get_min_max_dates(df)\n",
    "\n",
    "# print(\"Min date:\", min_date)\n",
    "# print(\"Max date:\", max_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_df_column_vs_time(df,column_name,time_column_name='Time'):\n",
    "\n",
    "    # Sample data\n",
    "    dates = df[time_column_name].dt.strftime('%Y-%m-%d %H:%M:%S').to_list()\n",
    "    values = df[column_name]\n",
    "\n",
    "    # Create a figure with a larger size\n",
    "    plt.figure(figsize=(7, 4))\n",
    "\n",
    "    # Create a line plot\n",
    "    plt.plot(dates, values)\n",
    "\n",
    "    # Rotate x-tick labels by 45 degrees and change their font size\n",
    "    plt.xticks(rotation=45, fontsize=12, ha='right')\n",
    "\n",
    "\n",
    "    # Use MaxNLocator to reduce the number of ticks\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=5))  # Adjust the number of bins as needed\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Date and Time')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Plot with Rotated and Formatted x-tick Labels')\n",
    "\n",
    "    # Add grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Adjust layout to prevent clipping of tick-labels\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Optionally adjust the subplots manually\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_float(s):\n",
    "    \"\"\"\n",
    "    Convert a string representation of a number in exponential format to a float.\n",
    "    \n",
    "    Parameters:\n",
    "    - s: String containing the number in exponential format.\n",
    "    \n",
    "    Returns:\n",
    "    - Float representation of the number.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the string contains a decimal point in the exponent part\n",
    "        if 'E+' in s or 'E-' in s or 'e+' in s or 'e-' in s:\n",
    "            parts = s.split('E') if 'E' in s else s.split('e')\n",
    "            \n",
    "            # Extract the base number and exponent\n",
    "            base = float(parts[0])\n",
    "            exponent = float(parts[1])\n",
    "            \n",
    "            # Adjust exponent if it contains a decimal point\n",
    "            if '.' in parts[1]:\n",
    "                exponent = int(float(parts[1]))  # Convert to int to remove decimal part\n",
    "            \n",
    "            # Calculate the final float value\n",
    "            result = base * (10 ** exponent)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        # If no 'E' or 'e' found, convert directly to float\n",
    "        return float(s)\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting '{s}' to float: {e}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FILE_AUX:\n",
    "    # def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_aux class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_AUX:\n",
    "\n",
    "    def __init__(self):\n",
    "        k = 1\n",
    "\n",
    "    def export_df(self,df, out_file_name, columns=None, start_line=0, num_lines=None):\n",
    "        \"\"\"\n",
    "        Exports a subset of a DataFrame to an Excel file.\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        out_file_name (str): The name of the output Excel file.\n",
    "        columns (list): List of columns to include in the export.\n",
    "        start_line (int): The starting line (index) from which to export.\n",
    "        num_lines (int): The number of lines (rows) to export.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        if (columns==None):\n",
    "            columns = df.columns\n",
    "\n",
    "        if (num_lines==None):\n",
    "            num_lines = df.shape[0]\n",
    "        # Select the desired subset of the DataFrame\n",
    "        subset_df = df[columns].iloc[start_line:start_line+num_lines-1]\n",
    "        print(subset_df.shape)\n",
    "        \n",
    "        # Export the subset to an Excel file\n",
    "\n",
    "        file_name, file_extension = os.path.splitext(out_file_name)\n",
    "\n",
    "        print(f'exporting {num_lines} lines from df to {out_file_name}')\n",
    "\n",
    "        if (file_extension=='.xlsx'):\n",
    "            subset_df.to_excel(out_file_name, index=False)\n",
    "            \n",
    "        elif (file_extension=='.csv'):\n",
    "            subset_df.to_csv(out_file_name, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_aux = DF_AUX()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# display functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(dict):\n",
    "    for key in dict.keys():\n",
    "        print(f'{key}:{dict[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vessels class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VESSELES:\n",
    "    # Class attribute\n",
    "    vehicle_count = 0\n",
    "\n",
    "    # Initializer / Instance attributes\n",
    "    def __init__(self):\n",
    "        self.data_dic = []  # It should be self.kuku to be an instance attribute\n",
    "        self.info_df = []\n",
    "        self.prob_MMSI = [];\n",
    "\n",
    "    def load_data(self, input_csv_file_name_full, columns_list_keep, min_date=None, max_date=None, reload=False):\n",
    "        # Convert the Time column from 'YYYYMMDD_HHMMSS' to 'YYYY-MM-DD HH:MM:SS'\n",
    "        pkl_file_name_full = input_csv_file_name_full.replace(\".csv\", \".pkl\")\n",
    "\n",
    "        df = load_or_create_df(input_csv_file_name_full, pkl_file_name_full, reload=reload)\n",
    "\n",
    "# in case the format has already changed\n",
    "        try:\n",
    "            df = convert_time_format(df, 'Time', '%Y%m%d_%H%M%S', '%Y-%m-%d %H:%M:%S')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        df = filter_df_by_date(df, min_date, max_date)\n",
    "\n",
    "        # Get a list of interesting columns\n",
    "        df = df[columns_list_keep]\n",
    "\n",
    "        print(f'df has {df.shape[0]} lines./ncolumns are:{df.columns.to_list()}')\n",
    "        return df\n",
    "\n",
    "    def create_data_dic(self,df):\n",
    "        print('creating data_dic')\n",
    "        grouped = df.groupby('MMSI')\n",
    "\n",
    "        # Create a dictionary to store each vessel's data\n",
    "        self.data_dic = {MMSI: group for MMSI, group in grouped}\n",
    "        return (self.data_dic)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def create_info_df(self,data_dic=None, min_data_len_thresh=2,to_print = True,num_lines = None):\n",
    "        print('create info_df')\n",
    "\n",
    "        info_df = pd.DataFrame()\n",
    "        prob_MMSI = []\n",
    "        MMSI_list = list(self.data_dic.keys())\n",
    "\n",
    "        if (num_lines != None):\n",
    "            MMSI_list = MMSI_list[:num_lines]\n",
    "\n",
    "            \n",
    "        if (data_dic==None):\n",
    "            data_dic = self.data_dic\n",
    "\n",
    "\n",
    "        for i, vessel_MMSI in enumerate(MMSI_list):\n",
    "            if (i % 1000 == 0):\n",
    "                print(f'processing MMSI {i} out of {len(MMSI_list)}')\n",
    "            vessel_data = get_vessel_data(data_dic, vessel_MMSI)  # Assuming get_vessel_data is defined elsewhere\n",
    "            \n",
    "            if (vessel_data.shape[0] < min_data_len_thresh):\n",
    "                prob_MMSI.append(vessel_MMSI)\n",
    "            else:\n",
    "                vessels_df_line = pd.DataFrame(get_vessel_data_stats(vessel_data), index=[vessel_MMSI])\n",
    "                info_df = pd.concat([info_df, vessels_df_line])\n",
    "\n",
    "        info_df = info_df.sort_values(by='len', ascending=False)\n",
    "\n",
    "        if (to_print):\n",
    "            print (f\"total number of MMSI:{len(MMSI_list)}\")\n",
    "            print (f\"{info_df.shape[0]} MMSI's passed\")\n",
    "            print (f\"{len(prob_MMSI)} MMSI's failed\")\n",
    "\n",
    "\n",
    "        self.prob_MMSI = prob_MMSI\n",
    "        self.info_df=info_df\n",
    "\n",
    "        return info_df,prob_MMSI  # Corrected return statement\n",
    "\n",
    "\n",
    "    def get_info_df_summary(self):\n",
    "        info_df_summary = {}\n",
    "\n",
    "        for column in (vessels_info_df.columns):\n",
    "            info_df_summary[column] = (self.info_df[column].min(),self.info_df[column].max())\n",
    "\n",
    "        print_dict(info_df_summary)\n",
    "\n",
    "        return \n",
    "\n",
    "\n",
    "    def get_vessel_data(self,vessel_MMSI,vessel_data_dic = None,to_print=False):\n",
    "        \n",
    "        if (vessel_data_dic==None):\n",
    "            vessel_data_dic = self.data_dic\n",
    "\n",
    "        if (1):\n",
    "        # try:\n",
    "            vessel_data = vessel_data_dic[vessel_MMSI]\n",
    "            print(vessel_data.shape)\n",
    "            # repeat missing ID values\n",
    "            ID_columns = ['IMO','Vessel_Name','Ship_Type']\n",
    "            for ID_column in ID_columns:\n",
    "                data = vessel_data[ID_column].loc[vessel_data[ID_column].notna()]\n",
    "                data = data.unique()\n",
    "                if (isinstance(data,str)):\n",
    "                    data = data.strip()\n",
    "                # print(vessel_data.shape)\n",
    "                vessel_data = repeat_single_value_in_column(vessel_data,data,ID_column)\n",
    "                # print(vessel_data.shape)\n",
    "                \n",
    "                if (vessel_data.empty):\n",
    "                    if (to_print is True):\n",
    "                        print(f'failed to create data base for vessel_MMSI={vessel_MMSI}')\n",
    "\n",
    "                    return vessel_data\n",
    "\n",
    "\n",
    "            # take only the lines where there is a Longitude\n",
    "            # print(vessel_data.shape)\n",
    "\n",
    "            vessel_data = vessel_data[vessel_data['Longitude'].notna()]\n",
    "\n",
    "    # handle exponent represntations \n",
    "            vessel_data.loc[:, 'Latitude'] = vessel_data['Latitude'].apply(convert_to_float)\n",
    "            vessel_data.loc[:, 'Longitude'] = vessel_data['Longitude'].apply(convert_to_float)\n",
    "            \n",
    "            # print(vessel_data.shape)\n",
    "    \n",
    "            # sort data by time\n",
    "            vessel_data = vessel_data.sort_values(by='Time')\n",
    "            # print('sucess')\n",
    "        # except:\n",
    "        #     vessel_data = pd.DataFrame()\n",
    "        #     # sys.exit(1)   \n",
    "        return vessel_data\n",
    "\n",
    "\n",
    "vessels = VESSELES()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init vessles class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the VESSELES class\n",
    "vessels = VESSELES()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_or_create_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m vessels\u001b[38;5;241m.\u001b[39mload_data(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_csv_file_name_full\u001b[39m\u001b[38;5;124m'\u001b[39m],columns_list_keep\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns_list_keep\u001b[39m\u001b[38;5;124m'\u001b[39m],reload\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreload\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\gilad\\work\\tip_and_que\\code\\tip-and-que-ships-personal\\vessels.py:31\u001b[0m, in \u001b[0;36mVESSELES.load_data\u001b[1;34m(self, input_csv_file_name_full, columns_list_keep, min_date, max_date, reload)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_csv_file_name_full, columns_list_keep, min_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;66;03m# Convert the Time column from 'YYYYMMDD_HHMMSS' to 'YYYY-MM-DD HH:MM:SS'\u001b[39;00m\n\u001b[0;32m     29\u001b[0m         pkl_file_name_full \u001b[38;5;241m=\u001b[39m input_csv_file_name_full\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m         df \u001b[38;5;241m=\u001b[39m load_or_create_df(input_csv_file_name_full, pkl_file_name_full, reload\u001b[38;5;241m=\u001b[39mreload)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# in case the format has already changed\u001b[39;00m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_or_create_df' is not defined"
     ]
    }
   ],
   "source": [
    "df = vessels.load_data(params['input_csv_file_name_full'],columns_list_keep=params['columns_list_keep'],reload=params['reload'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create debug data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_min_max_dates(df)\n",
    "# filter_dic = {'Time':['between',('2023-02-01 00:00:01','2023-02-01 12:00:01')]}\n",
    "\n",
    "# df_filt = filter_df(df,filter_dic)\n",
    "\n",
    "\n",
    "# get_min_max_dates(df_filt)\n",
    "# print(df_filt.shape)\n",
    "# df_aux.export_df(df_filt,'debug_data_base.csv') \n",
    "# df1 = vessels.load_data('debug_data_base.csv',columns_list_keep=params['columns_list_keep'],reload=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to exell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (params['export_to_excel']):\n",
    "    df_aux.export_df(df, os.path.basename(params['input_csv_file_name_full'].replace('csv','xlsx')),num_lines=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create data_dic and info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating data_dic\n",
      "create info_df\n",
      "processing MMSI 0 out of 100\n",
      "total number of MMSI:100\n",
      "28 MMSI's passed\n",
      "72 MMSI's failed\n",
      "len:(8, 131)\n",
      "min_time:('2023-02-01 00:00:11', '2023-02-01 06:04:40')\n",
      "max_time:('2023-02-01 06:21:11', '2023-02-01 11:59:53')\n",
      "total_time:(Timedelta('0 days 04:06:01'), Timedelta('0 days 11:54:32'))\n",
      "min_time_diff[mins]:(0, 12)\n",
      "max_time_diff[mins]:(10, 187)\n",
      "mean_time_diff[mins]:(5, 35)\n",
      "min_Longitude:(25.7783333333, 55.5016666667)\n",
      "max_Longitude:(25.7796666667, 55.50321)\n",
      "min_Latitude:(23.9133333333, 36.4483333333)\n",
      "max_Latitude:(23.94, 36.4495)\n",
      "span_Longitude:(4.166659999782496e-05, 2.3704833333999957)\n",
      "span_Latitude:(6.49999999993156e-05, 1.2321999999999989)\n"
     ]
    }
   ],
   "source": [
    "vesseles_data_dic = vessels.create_data_dic(df)        \n",
    "vessels_info_df,prob_MMSI = vessels.create_info_df(num_lines=100)\n",
    "vessels.get_info_df_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:1\n"
     ]
    }
   ],
   "source": [
    "dict = {'A':1}\n",
    "print_dict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "      <th>min_time</th>\n",
       "      <th>max_time</th>\n",
       "      <th>total_time</th>\n",
       "      <th>min_time_diff[mins]</th>\n",
       "      <th>max_time_diff[mins]</th>\n",
       "      <th>mean_time_diff[mins]</th>\n",
       "      <th>min_Longitude</th>\n",
       "      <th>max_Longitude</th>\n",
       "      <th>min_Latitude</th>\n",
       "      <th>max_Latitude</th>\n",
       "      <th>span_Longitude</th>\n",
       "      <th>span_Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12345678</th>\n",
       "      <td>131</td>\n",
       "      <td>2023-02-01 00:05:31</td>\n",
       "      <td>2023-02-01 11:59:52</td>\n",
       "      <td>0 days 11:54:21</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>55.265250</td>\n",
       "      <td>55.265338</td>\n",
       "      <td>25.262010</td>\n",
       "      <td>25.262133</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209489000</th>\n",
       "      <td>126</td>\n",
       "      <td>2023-02-01 00:05:19</td>\n",
       "      <td>2023-02-01 11:50:09</td>\n",
       "      <td>0 days 11:44:50</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>25.778333</td>\n",
       "      <td>25.779667</td>\n",
       "      <td>36.448333</td>\n",
       "      <td>36.449500</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.001167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209700000</th>\n",
       "      <td>121</td>\n",
       "      <td>2023-02-01 00:03:36</td>\n",
       "      <td>2023-02-01 11:57:09</td>\n",
       "      <td>0 days 11:53:33</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>55.007097</td>\n",
       "      <td>55.327507</td>\n",
       "      <td>25.025000</td>\n",
       "      <td>25.438465</td>\n",
       "      <td>0.320410</td>\n",
       "      <td>0.413465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210282000</th>\n",
       "      <td>90</td>\n",
       "      <td>2023-02-01 00:03:48</td>\n",
       "      <td>2023-02-01 11:52:58</td>\n",
       "      <td>0 days 11:49:10</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>33.010000</td>\n",
       "      <td>33.183148</td>\n",
       "      <td>34.646667</td>\n",
       "      <td>34.688695</td>\n",
       "      <td>0.173148</td>\n",
       "      <td>0.042028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209492000</th>\n",
       "      <td>89</td>\n",
       "      <td>2023-02-01 00:05:59</td>\n",
       "      <td>2023-02-01 11:57:27</td>\n",
       "      <td>0 days 11:51:28</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>33.316667</td>\n",
       "      <td>33.318470</td>\n",
       "      <td>34.716667</td>\n",
       "      <td>34.718615</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.001948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205231000</th>\n",
       "      <td>86</td>\n",
       "      <td>2023-02-01 00:08:20</td>\n",
       "      <td>2023-02-01 11:53:23</td>\n",
       "      <td>0 days 11:45:03</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>50.653333</td>\n",
       "      <td>50.653482</td>\n",
       "      <td>26.211667</td>\n",
       "      <td>26.212947</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.001280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209815000</th>\n",
       "      <td>83</td>\n",
       "      <td>2023-02-01 00:04:20</td>\n",
       "      <td>2023-02-01 11:58:17</td>\n",
       "      <td>0 days 11:53:57</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>33.013310</td>\n",
       "      <td>33.013430</td>\n",
       "      <td>34.644947</td>\n",
       "      <td>34.645023</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209511000</th>\n",
       "      <td>75</td>\n",
       "      <td>2023-02-01 02:02:00</td>\n",
       "      <td>2023-02-01 11:53:56</td>\n",
       "      <td>0 days 09:51:56</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>33.019828</td>\n",
       "      <td>33.730132</td>\n",
       "      <td>33.947252</td>\n",
       "      <td>34.657088</td>\n",
       "      <td>0.710303</td>\n",
       "      <td>0.709837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209343000</th>\n",
       "      <td>73</td>\n",
       "      <td>2023-02-01 00:00:33</td>\n",
       "      <td>2023-02-01 11:53:23</td>\n",
       "      <td>0 days 11:52:50</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>33.009005</td>\n",
       "      <td>33.009050</td>\n",
       "      <td>34.645058</td>\n",
       "      <td>34.645123</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209773000</th>\n",
       "      <td>71</td>\n",
       "      <td>2023-02-01 00:00:11</td>\n",
       "      <td>2023-02-01 11:54:43</td>\n",
       "      <td>0 days 11:54:32</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>10</td>\n",
       "      <td>38.201667</td>\n",
       "      <td>38.248467</td>\n",
       "      <td>23.913333</td>\n",
       "      <td>23.940000</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.026667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210255000</th>\n",
       "      <td>67</td>\n",
       "      <td>2023-02-01 04:46:19</td>\n",
       "      <td>2023-02-01 11:09:38</td>\n",
       "      <td>0 days 06:23:19</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>33.031667</td>\n",
       "      <td>33.324473</td>\n",
       "      <td>34.570000</td>\n",
       "      <td>34.718840</td>\n",
       "      <td>0.292807</td>\n",
       "      <td>0.148840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210001000</th>\n",
       "      <td>65</td>\n",
       "      <td>2023-02-01 00:10:52</td>\n",
       "      <td>2023-02-01 11:59:53</td>\n",
       "      <td>0 days 11:49:01</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>33.010378</td>\n",
       "      <td>33.070040</td>\n",
       "      <td>34.647307</td>\n",
       "      <td>34.664963</td>\n",
       "      <td>0.059662</td>\n",
       "      <td>0.017657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209005000</th>\n",
       "      <td>60</td>\n",
       "      <td>2023-02-01 00:02:12</td>\n",
       "      <td>2023-02-01 11:50:15</td>\n",
       "      <td>0 days 11:48:03</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>34.643680</td>\n",
       "      <td>34.643730</td>\n",
       "      <td>31.829147</td>\n",
       "      <td>31.829215</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209048000</th>\n",
       "      <td>60</td>\n",
       "      <td>2023-02-01 00:07:50</td>\n",
       "      <td>2023-02-01 11:58:51</td>\n",
       "      <td>0 days 11:51:01</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>33.080380</td>\n",
       "      <td>33.081593</td>\n",
       "      <td>34.675018</td>\n",
       "      <td>34.677142</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.002123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209969000</th>\n",
       "      <td>60</td>\n",
       "      <td>2023-02-01 00:09:50</td>\n",
       "      <td>2023-02-01 11:57:51</td>\n",
       "      <td>0 days 11:48:01</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>35.006258</td>\n",
       "      <td>35.006342</td>\n",
       "      <td>32.819605</td>\n",
       "      <td>32.819690</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205481000</th>\n",
       "      <td>58</td>\n",
       "      <td>2023-02-01 00:10:55</td>\n",
       "      <td>2023-02-01 11:52:54</td>\n",
       "      <td>0 days 11:41:59</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>34.995792</td>\n",
       "      <td>34.995833</td>\n",
       "      <td>29.515423</td>\n",
       "      <td>29.515500</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209352000</th>\n",
       "      <td>56</td>\n",
       "      <td>2023-02-01 00:07:09</td>\n",
       "      <td>2023-02-01 11:54:08</td>\n",
       "      <td>0 days 11:46:59</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>33.029635</td>\n",
       "      <td>33.029717</td>\n",
       "      <td>34.664420</td>\n",
       "      <td>34.664520</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576659</th>\n",
       "      <td>55</td>\n",
       "      <td>2023-02-01 00:06:45</td>\n",
       "      <td>2023-02-01 11:56:45</td>\n",
       "      <td>0 days 11:50:00</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>13</td>\n",
       "      <td>55.046280</td>\n",
       "      <td>55.063313</td>\n",
       "      <td>24.986723</td>\n",
       "      <td>25.017448</td>\n",
       "      <td>0.017033</td>\n",
       "      <td>0.030725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209258000</th>\n",
       "      <td>54</td>\n",
       "      <td>2023-02-01 00:05:29</td>\n",
       "      <td>2023-02-01 11:58:36</td>\n",
       "      <td>0 days 11:53:07</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>55.040842</td>\n",
       "      <td>55.059215</td>\n",
       "      <td>24.989998</td>\n",
       "      <td>25.022705</td>\n",
       "      <td>0.018373</td>\n",
       "      <td>0.032707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205706000</th>\n",
       "      <td>52</td>\n",
       "      <td>2023-02-01 00:03:58</td>\n",
       "      <td>2023-02-01 11:12:19</td>\n",
       "      <td>0 days 11:08:21</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>51.633333</td>\n",
       "      <td>54.003817</td>\n",
       "      <td>25.954467</td>\n",
       "      <td>27.186667</td>\n",
       "      <td>2.370483</td>\n",
       "      <td>1.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210211000</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-02-01 00:01:09</td>\n",
       "      <td>2023-02-01 10:45:48</td>\n",
       "      <td>0 days 10:44:39</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>13</td>\n",
       "      <td>55.501667</td>\n",
       "      <td>55.503210</td>\n",
       "      <td>25.468333</td>\n",
       "      <td>25.468528</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210297000</th>\n",
       "      <td>46</td>\n",
       "      <td>2023-02-01 03:59:03</td>\n",
       "      <td>2023-02-01 11:58:23</td>\n",
       "      <td>0 days 07:59:20</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>11</td>\n",
       "      <td>31.495267</td>\n",
       "      <td>33.061478</td>\n",
       "      <td>34.535848</td>\n",
       "      <td>35.117362</td>\n",
       "      <td>1.566212</td>\n",
       "      <td>0.581513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9129852</th>\n",
       "      <td>41</td>\n",
       "      <td>2023-02-01 03:37:38</td>\n",
       "      <td>2023-02-01 11:51:11</td>\n",
       "      <td>0 days 08:13:33</td>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>33.315495</td>\n",
       "      <td>33.318292</td>\n",
       "      <td>34.716488</td>\n",
       "      <td>34.718885</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.002397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200000002</th>\n",
       "      <td>39</td>\n",
       "      <td>2023-02-01 00:08:19</td>\n",
       "      <td>2023-02-01 11:38:17</td>\n",
       "      <td>0 days 11:29:58</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>18</td>\n",
       "      <td>34.613333</td>\n",
       "      <td>34.613412</td>\n",
       "      <td>31.523333</td>\n",
       "      <td>31.524293</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998001</th>\n",
       "      <td>36</td>\n",
       "      <td>2023-02-01 04:06:14</td>\n",
       "      <td>2023-02-01 11:53:14</td>\n",
       "      <td>0 days 07:47:00</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>13</td>\n",
       "      <td>48.341453</td>\n",
       "      <td>48.905210</td>\n",
       "      <td>28.989958</td>\n",
       "      <td>29.406532</td>\n",
       "      <td>0.563757</td>\n",
       "      <td>0.416573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209617000</th>\n",
       "      <td>22</td>\n",
       "      <td>2023-02-01 06:04:40</td>\n",
       "      <td>2023-02-01 11:52:22</td>\n",
       "      <td>0 days 05:47:42</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>16</td>\n",
       "      <td>47.978717</td>\n",
       "      <td>48.193383</td>\n",
       "      <td>29.997900</td>\n",
       "      <td>30.031883</td>\n",
       "      <td>0.214667</td>\n",
       "      <td>0.033983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4381234</th>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-01 00:00:57</td>\n",
       "      <td>2023-02-01 11:34:35</td>\n",
       "      <td>0 days 11:33:38</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>35</td>\n",
       "      <td>34.995083</td>\n",
       "      <td>34.995183</td>\n",
       "      <td>29.546133</td>\n",
       "      <td>29.546267</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210364000</th>\n",
       "      <td>8</td>\n",
       "      <td>2023-02-01 02:15:10</td>\n",
       "      <td>2023-02-01 06:21:11</td>\n",
       "      <td>0 days 04:06:01</td>\n",
       "      <td>12</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>49.200217</td>\n",
       "      <td>49.200507</td>\n",
       "      <td>29.863830</td>\n",
       "      <td>29.865068</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.001238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           len             min_time             max_time      total_time  \\\n",
       "12345678   131  2023-02-01 00:05:31  2023-02-01 11:59:52 0 days 11:54:21   \n",
       "209489000  126  2023-02-01 00:05:19  2023-02-01 11:50:09 0 days 11:44:50   \n",
       "209700000  121  2023-02-01 00:03:36  2023-02-01 11:57:09 0 days 11:53:33   \n",
       "210282000   90  2023-02-01 00:03:48  2023-02-01 11:52:58 0 days 11:49:10   \n",
       "209492000   89  2023-02-01 00:05:59  2023-02-01 11:57:27 0 days 11:51:28   \n",
       "205231000   86  2023-02-01 00:08:20  2023-02-01 11:53:23 0 days 11:45:03   \n",
       "209815000   83  2023-02-01 00:04:20  2023-02-01 11:58:17 0 days 11:53:57   \n",
       "209511000   75  2023-02-01 02:02:00  2023-02-01 11:53:56 0 days 09:51:56   \n",
       "209343000   73  2023-02-01 00:00:33  2023-02-01 11:53:23 0 days 11:52:50   \n",
       "209773000   71  2023-02-01 00:00:11  2023-02-01 11:54:43 0 days 11:54:32   \n",
       "210255000   67  2023-02-01 04:46:19  2023-02-01 11:09:38 0 days 06:23:19   \n",
       "210001000   65  2023-02-01 00:10:52  2023-02-01 11:59:53 0 days 11:49:01   \n",
       "209005000   60  2023-02-01 00:02:12  2023-02-01 11:50:15 0 days 11:48:03   \n",
       "209048000   60  2023-02-01 00:07:50  2023-02-01 11:58:51 0 days 11:51:01   \n",
       "209969000   60  2023-02-01 00:09:50  2023-02-01 11:57:51 0 days 11:48:01   \n",
       "205481000   58  2023-02-01 00:10:55  2023-02-01 11:52:54 0 days 11:41:59   \n",
       "209352000   56  2023-02-01 00:07:09  2023-02-01 11:54:08 0 days 11:46:59   \n",
       "2576659     55  2023-02-01 00:06:45  2023-02-01 11:56:45 0 days 11:50:00   \n",
       "209258000   54  2023-02-01 00:05:29  2023-02-01 11:58:36 0 days 11:53:07   \n",
       "205706000   52  2023-02-01 00:03:58  2023-02-01 11:12:19 0 days 11:08:21   \n",
       "210211000   51  2023-02-01 00:01:09  2023-02-01 10:45:48 0 days 10:44:39   \n",
       "210297000   46  2023-02-01 03:59:03  2023-02-01 11:58:23 0 days 07:59:20   \n",
       "9129852     41  2023-02-01 03:37:38  2023-02-01 11:51:11 0 days 08:13:33   \n",
       "200000002   39  2023-02-01 00:08:19  2023-02-01 11:38:17 0 days 11:29:58   \n",
       "41998001    36  2023-02-01 04:06:14  2023-02-01 11:53:14 0 days 07:47:00   \n",
       "209617000   22  2023-02-01 06:04:40  2023-02-01 11:52:22 0 days 05:47:42   \n",
       "4381234     21  2023-02-01 00:00:57  2023-02-01 11:34:35 0 days 11:33:38   \n",
       "210364000    8  2023-02-01 02:15:10  2023-02-01 06:21:11 0 days 04:06:01   \n",
       "\n",
       "           min_time_diff[mins]  max_time_diff[mins]  mean_time_diff[mins]  \\\n",
       "12345678                     0                   10                     5   \n",
       "209489000                    0                   16                     6   \n",
       "209700000                    0                   24                     6   \n",
       "210282000                    0                   30                     8   \n",
       "209492000                    0                   15                     8   \n",
       "205231000                    0                   14                     8   \n",
       "209815000                    1                   11                     9   \n",
       "209511000                    0                   12                     8   \n",
       "209343000                    3                   17                    10   \n",
       "209773000                    0                   84                    10   \n",
       "210255000                    0                   20                     6   \n",
       "210001000                    0                   15                    11   \n",
       "209005000                   12                   12                    12   \n",
       "209048000                   12                   15                    12   \n",
       "209969000                   12                   12                    12   \n",
       "205481000                    9                   21                    12   \n",
       "209352000                   10                   25                    13   \n",
       "2576659                      0                  155                    13   \n",
       "209258000                    1                   27                    13   \n",
       "205706000                    0                   60                    13   \n",
       "210211000                    0                   57                    13   \n",
       "210297000                    0                  187                    11   \n",
       "9129852                     10                   27                    12   \n",
       "200000002                    2                   75                    18   \n",
       "41998001                     0                   52                    13   \n",
       "209617000                    0                   99                    16   \n",
       "4381234                      0                  164                    35   \n",
       "210364000                   12                   69                    35   \n",
       "\n",
       "           min_Longitude  max_Longitude  min_Latitude  max_Latitude  \\\n",
       "12345678       55.265250      55.265338     25.262010     25.262133   \n",
       "209489000      25.778333      25.779667     36.448333     36.449500   \n",
       "209700000      55.007097      55.327507     25.025000     25.438465   \n",
       "210282000      33.010000      33.183148     34.646667     34.688695   \n",
       "209492000      33.316667      33.318470     34.716667     34.718615   \n",
       "205231000      50.653333      50.653482     26.211667     26.212947   \n",
       "209815000      33.013310      33.013430     34.644947     34.645023   \n",
       "209511000      33.019828      33.730132     33.947252     34.657088   \n",
       "209343000      33.009005      33.009050     34.645058     34.645123   \n",
       "209773000      38.201667      38.248467     23.913333     23.940000   \n",
       "210255000      33.031667      33.324473     34.570000     34.718840   \n",
       "210001000      33.010378      33.070040     34.647307     34.664963   \n",
       "209005000      34.643680      34.643730     31.829147     31.829215   \n",
       "209048000      33.080380      33.081593     34.675018     34.677142   \n",
       "209969000      35.006258      35.006342     32.819605     32.819690   \n",
       "205481000      34.995792      34.995833     29.515423     29.515500   \n",
       "209352000      33.029635      33.029717     34.664420     34.664520   \n",
       "2576659        55.046280      55.063313     24.986723     25.017448   \n",
       "209258000      55.040842      55.059215     24.989998     25.022705   \n",
       "205706000      51.633333      54.003817     25.954467     27.186667   \n",
       "210211000      55.501667      55.503210     25.468333     25.468528   \n",
       "210297000      31.495267      33.061478     34.535848     35.117362   \n",
       "9129852        33.315495      33.318292     34.716488     34.718885   \n",
       "200000002      34.613333      34.613412     31.523333     31.524293   \n",
       "41998001       48.341453      48.905210     28.989958     29.406532   \n",
       "209617000      47.978717      48.193383     29.997900     30.031883   \n",
       "4381234        34.995083      34.995183     29.546133     29.546267   \n",
       "210364000      49.200217      49.200507     29.863830     29.865068   \n",
       "\n",
       "           span_Longitude  span_Latitude  \n",
       "12345678         0.000088       0.000123  \n",
       "209489000        0.001333       0.001167  \n",
       "209700000        0.320410       0.413465  \n",
       "210282000        0.173148       0.042028  \n",
       "209492000        0.001803       0.001948  \n",
       "205231000        0.000148       0.001280  \n",
       "209815000        0.000120       0.000077  \n",
       "209511000        0.710303       0.709837  \n",
       "209343000        0.000045       0.000065  \n",
       "209773000        0.046800       0.026667  \n",
       "210255000        0.292807       0.148840  \n",
       "210001000        0.059662       0.017657  \n",
       "209005000        0.000050       0.000068  \n",
       "209048000        0.001213       0.002123  \n",
       "209969000        0.000083       0.000085  \n",
       "205481000        0.000042       0.000077  \n",
       "209352000        0.000082       0.000100  \n",
       "2576659          0.017033       0.030725  \n",
       "209258000        0.018373       0.032707  \n",
       "205706000        2.370483       1.232200  \n",
       "210211000        0.001543       0.000195  \n",
       "210297000        1.566212       0.581513  \n",
       "9129852          0.002797       0.002397  \n",
       "200000002        0.000078       0.000960  \n",
       "41998001         0.563757       0.416573  \n",
       "209617000        0.214667       0.033983  \n",
       "4381234          0.000100       0.000133  \n",
       "210364000        0.000290       0.001238  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vessels_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get a vessel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>MMSI</th>\n",
       "      <th>IMO</th>\n",
       "      <th>Vessel_Name</th>\n",
       "      <th>Ship_Type</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Message_ID</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Heading</th>\n",
       "      <th>COG</th>\n",
       "      <th>Fixing_device</th>\n",
       "      <th>Destination_ID</th>\n",
       "      <th>offset1</th>\n",
       "      <th>Offset_2</th>\n",
       "      <th>Offset_3</th>\n",
       "      <th>Offset_4</th>\n",
       "      <th>ATON_type</th>\n",
       "      <th>ATON_name</th>\n",
       "      <th>GNSS_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284911</th>\n",
       "      <td>2023-02-01 00:03:36</td>\n",
       "      <td>209700000</td>\n",
       "      <td>9134139.0</td>\n",
       "      <td>DUBAI ALLIANCE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.068333</td>\n",
       "      <td>25.025</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>2023-02-01 00:23:38</td>\n",
       "      <td>209700000</td>\n",
       "      <td>9134139.0</td>\n",
       "      <td>DUBAI ALLIANCE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.069923</td>\n",
       "      <td>25.026493</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>128.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142104</th>\n",
       "      <td>2023-02-01 00:24:31</td>\n",
       "      <td>209700000</td>\n",
       "      <td>9134139.0</td>\n",
       "      <td>DUBAI ALLIANCE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.068333</td>\n",
       "      <td>25.025</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26086</th>\n",
       "      <td>2023-02-01 00:35:39</td>\n",
       "      <td>209700000</td>\n",
       "      <td>9134139.0</td>\n",
       "      <td>DUBAI ALLIANCE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.069907</td>\n",
       "      <td>25.026482</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288114</th>\n",
       "      <td>2023-02-01 00:42:35</td>\n",
       "      <td>209700000</td>\n",
       "      <td>9134139.0</td>\n",
       "      <td>DUBAI ALLIANCE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.068333</td>\n",
       "      <td>25.025</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143619</th>\n",
       "      <td>2023-02-01 11:44:58</td>\n",
       "      <td>209700000</td>\n",
       "      <td>9134139.0</td>\n",
       "      <td>DUBAI ALLIANCE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.287997</td>\n",
       "      <td>25.432198</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73938</th>\n",
       "      <td>2023-02-01 11:45:09</td>\n",
       "      <td>209700000</td>\n",
       "      <td>9134139.0</td>\n",
       "      <td>DUBAI ALLIANCE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.288593</td>\n",
       "      <td>25.432292</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>79.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109270</th>\n",
       "      <td>2023-02-01 11:51:29</td>\n",
       "      <td>209700000</td>\n",
       "      <td>9134139.0</td>\n",
       "      <td>DUBAI ALLIANCE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.309043</td>\n",
       "      <td>25.43556</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143628</th>\n",
       "      <td>2023-02-01 11:55:09</td>\n",
       "      <td>209700000</td>\n",
       "      <td>9134139.0</td>\n",
       "      <td>DUBAI ALLIANCE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.320997</td>\n",
       "      <td>25.437443</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93068</th>\n",
       "      <td>2023-02-01 11:57:09</td>\n",
       "      <td>209700000</td>\n",
       "      <td>9134139.0</td>\n",
       "      <td>DUBAI ALLIANCE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.327507</td>\n",
       "      <td>25.438465</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>79.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Time       MMSI        IMO           Vessel_Name  \\\n",
       "284911 2023-02-01 00:03:36  209700000  9134139.0  DUBAI ALLIANCE         \n",
       "9012   2023-02-01 00:23:38  209700000  9134139.0  DUBAI ALLIANCE         \n",
       "142104 2023-02-01 00:24:31  209700000  9134139.0  DUBAI ALLIANCE         \n",
       "26086  2023-02-01 00:35:39  209700000  9134139.0  DUBAI ALLIANCE         \n",
       "288114 2023-02-01 00:42:35  209700000  9134139.0  DUBAI ALLIANCE         \n",
       "...                    ...        ...        ...                   ...   \n",
       "143619 2023-02-01 11:44:58  209700000  9134139.0  DUBAI ALLIANCE         \n",
       "73938  2023-02-01 11:45:09  209700000  9134139.0  DUBAI ALLIANCE         \n",
       "109270 2023-02-01 11:51:29  209700000  9134139.0  DUBAI ALLIANCE         \n",
       "143628 2023-02-01 11:55:09  209700000  9134139.0  DUBAI ALLIANCE         \n",
       "93068  2023-02-01 11:57:09  209700000  9134139.0  DUBAI ALLIANCE         \n",
       "\n",
       "        Ship_Type  Longitude   Latitude  Message_ID  Accuracy  Heading    COG  \\\n",
       "284911       70.0  55.068333     25.025          27       1.0      NaN   21.0   \n",
       "9012         70.0  55.069923  25.026493           3       1.0    258.0  128.9   \n",
       "142104       70.0  55.068333     25.025          27       1.0      NaN   93.0   \n",
       "26086        70.0  55.069907  25.026482           3       1.0    259.0    NaN   \n",
       "288114       70.0  55.068333     25.025          27       1.0      NaN    6.0   \n",
       "...           ...        ...        ...         ...       ...      ...    ...   \n",
       "143619       70.0  55.287997  25.432198           1       1.0     81.0   80.0   \n",
       "73938        70.0  55.288593  25.432292           3       1.0     81.0   79.8   \n",
       "109270       70.0  55.309043   25.43556          11       1.0      NaN    NaN   \n",
       "143628       70.0  55.320997  25.437443           1       1.0     80.0   80.3   \n",
       "93068        70.0  55.327507  25.438465           3       1.0     80.0   79.8   \n",
       "\n",
       "        Fixing_device  Destination_ID  offset1  Offset_2  Offset_3  Offset_4  \\\n",
       "284911            NaN             NaN      NaN       NaN       NaN       NaN   \n",
       "9012              NaN             NaN      NaN       NaN       NaN       NaN   \n",
       "142104            NaN             NaN      NaN       NaN       NaN       NaN   \n",
       "26086             NaN             NaN      NaN       NaN       NaN       NaN   \n",
       "288114            NaN             NaN      NaN       NaN       NaN       NaN   \n",
       "...               ...             ...      ...       ...       ...       ...   \n",
       "143619            NaN             NaN      NaN       NaN       NaN       NaN   \n",
       "73938             NaN             NaN      NaN       NaN       NaN       NaN   \n",
       "109270           15.0             NaN      NaN       NaN       NaN       NaN   \n",
       "143628            NaN             NaN      NaN       NaN       NaN       NaN   \n",
       "93068             NaN             NaN      NaN       NaN       NaN       NaN   \n",
       "\n",
       "        ATON_type ATON_name  GNSS_status  \n",
       "284911        NaN       NaN          0.0  \n",
       "9012          NaN       NaN          NaN  \n",
       "142104        NaN       NaN          0.0  \n",
       "26086         NaN       NaN          NaN  \n",
       "288114        NaN       NaN          0.0  \n",
       "...           ...       ...          ...  \n",
       "143619        NaN       NaN          NaN  \n",
       "73938         NaN       NaN          NaN  \n",
       "109270        NaN       NaN          NaN  \n",
       "143628        NaN       NaN          NaN  \n",
       "93068         NaN       NaN          NaN  \n",
       "\n",
       "[121 rows x 20 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vessel_data = get_vessel_data(vessels.data_dic,422)\n",
    "vessel_data = vessels.get_vessel_data(vessels_info_df.index[2])\n",
    "vessel_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot some statistis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vessels_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 44\u001b[0m\n\u001b[0;32m     39\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     42\u001b[0m columns_to_plot \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield3\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 44\u001b[0m plot_vessels_df_statistics(vessels_df, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlen\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_time_diff[mins]\u001b[39m\u001b[38;5;124m'\u001b[39m], bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vessels_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_vessels_df_statistics(df, columns, bins=10):\n",
    "    \"\"\"\n",
    "    Plots histograms of specified columns in a DataFrame as subplots.\n",
    "\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    Parameters:\n",
    "    columns (list): List of column names to plot.\n",
    "    bins (int): Number of bins for the histograms.\n",
    "    \"\"\"\n",
    "    num_columns = len(columns)\n",
    "    num_rows = (num_columns + 1) // 2  # Calculate number of rows needed for subplots\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, 2, figsize=(12, num_rows * 4))\n",
    "    axes = axes.flatten()  # Flatten the axes array to easily iterate over it\n",
    "\n",
    "    for i, column in enumerate(columns):\n",
    "        try:\n",
    "            if column in df.columns:\n",
    "                axes[i].hist(df[column], bins=bins, edgecolor='black')\n",
    "                axes[i].set_title(f'Histogram of {column}')\n",
    "                axes[i].set_xlabel(column)\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "            else:\n",
    "                axes[i].text(0.5, 0.5, f'Column {column} not found', ha='center', va='center')\n",
    "                axes[i].set_title(f'Histogram of {column}')\n",
    "                axes[i].set_xlabel(column)\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "        except:\n",
    "            print(f'could not plot {column}')\n",
    "            \n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "columns_to_plot = ['field1', 'field2', 'field3']\n",
    "\n",
    "plot_vessels_df_statistics(vessels_df, ['len','min_time_diff[mins]'], bins=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filtering the vessles Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dic = {\n",
    "        'len':['between',(700,800)],\n",
    "        'min_Longitude': ('between', (40.0, 50.0)),        \n",
    "      #   'mean_time_diff[mins]':['==',13],\n",
    "        'min_time':['between',('2023-02-01 00:00:00','2023-02-01 00:01:39')]\n",
    "              }\n",
    "\n",
    "# filter_dic = {\n",
    "#           'max_time_diff[mins]':['<=',30]\n",
    "#           }\n",
    "\n",
    "vessels_df_filt = filter_df(vessels_info_df, filter_dic)\n",
    "# df_filt.shape\n",
    "vessels_df_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the vessel_data to a jason file\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_data_jason_file_path = '.\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_vessels_data_to_geojson(vessels_df_filt,vessel_data_dic,vessel_data_jason_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots of vessel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vessel_data = get_vessel_data(vessel_data_dic,MMSI)\n",
    "plot_df_column_vs_time(vessel_data,column_name='Longitude')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('MMSI')\n",
    "\n",
    "# Create a dictionary to store each vessel's data\n",
    "vessel_data_dic = {vessel_MMSI: group for vessel_MMSI, group in grouped}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
